{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63d7a471",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-4-be366fc91385>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-4-be366fc91385>\"\u001b[1;36m, line \u001b[1;32m3\u001b[0m\n\u001b[1;33m    pip install tensorflow_datasets\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#did not work\n",
    "#!pip install -q git+https://github.com/tensorflow/examples.git\n",
    "pip install tensorflow_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a68ac42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#coding in tensor flow 1.0\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import tensorflow_datasets as tfds\n",
    "# Construct a tf.data.Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "abe35e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mnist = tfds.load('mnist')\n",
    "#data set in C:\\Users\\Acer\\tensorflow_datasets\\mnist\\3.0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beacf00e",
   "metadata": {},
   "source": [
    "<h2>steps</h2>\n",
    "\n",
    "\n",
    "input > weight 2 > hidden layer 1 (activation function) > weights > hidden layer 2 (activation function) > weights > output layer\n",
    "\n",
    "compare output with intended output > cost function (cross entropy)\n",
    "optimization function (optimizer) > minimize cost (adamOptimizer ... SGD, AdaGrad\n",
    "\n",
    "back propagation\n",
    "\n",
    "feed forward + backprop = epoch\n",
    "\n",
    "code from https://pythonprogramming.net/train-test-tensorflow-deep-learning-tutorial/\n",
    "(old version, does not properly work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3418ec50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 5s 0us/step\n"
     ]
    }
   ],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dc073bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = tfds.load(name=\"mnist\", split=tfds.Split.TRAIN)\n",
    "\n",
    "#dataset, info = tfds.load(\"mnist\", with_info=True, as_supervised=True)\n",
    "\n",
    "#print(dataset)\n",
    "#mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot = True)\n",
    "\n",
    "# one hot encode for 10 MNIST classes\n",
    "#def my_one_hot(feature, label):\n",
    "#    return feature, tf.one_hot(label, depth=10)\n",
    "\n",
    "# load your data from tfds\n",
    "#mnist_train, train_info = tfds.load(name=\"mnist\", with_info=True, as_supervised=True, split=tfds.Split.TRAIN)\n",
    "\n",
    "# convert your labels in one-hot\n",
    "#mnist_train = mnist_train.map(my_one_hot)\n",
    "\n",
    "# you can batch your data here\n",
    "#mnist_train = mnist_train.batch(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "10e16b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch(num, data, labels):\n",
    "    '''\n",
    "    Return a total of `num` random samples and labels. \n",
    "    '''\n",
    "    idx = np.arange(0 , len(data))\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:num]\n",
    "    data_shuffle = [data[i] for i in idx]\n",
    "    labels_shuffle = [labels[i] for i in idx]\n",
    "\n",
    "    return np.asarray(data_shuffle), np.asarray(labels_shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0f2c5e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tree hidden layers\n",
    "n_nodes_hl1 = 500\n",
    "n_nodes_hl2 = 500\n",
    "n_nodes_hl3 = 500\n",
    "\n",
    "#number of classes\n",
    "n_classes = 10\n",
    "\n",
    "#not enougth of ram to load all at same time\n",
    "batch_size = 100\n",
    "\n",
    "#transform the matrix to a vector\n",
    "# for tf 1.0\n",
    "x = tf.placeholder('float', [None, 784])\n",
    "y = tf.placeholder('float')\n",
    "\n",
    "#in 2.0\n",
    "#https://gimoonnam.github.io/machinelearning/tensorFlow_1/\n",
    "#https://www.kaggle.com/general/201962"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e78fd9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_network_model(data):\n",
    "    \n",
    "    initializer = tf.random_normal_initializer()\n",
    "    \n",
    "    # input_data * weight + biases\n",
    "    #first hidden layer\n",
    "    hidden_1_layer = {'weights':  tf.Variable(tf.random_normal(shape=[784, n_nodes_hl1], dtype=tf.float32)),\n",
    "                         'biases': tf.Variable(tf.random_normal(shape=[n_nodes_hl1], dtype=tf.float32))}\n",
    "    \n",
    "    #second hidden layer\n",
    "    hidden_2_layer = {'weights':  tf.Variable(tf.random_normal(shape=[n_nodes_hl1, n_nodes_hl2], dtype=tf.float32)),\n",
    "                         'biases': tf.Variable(tf.random_normal(shape=[n_nodes_hl2], dtype=tf.float32))}\n",
    "        \n",
    "    #third hidden layer\n",
    "    hidden_3_layer = {'weights':  tf.Variable(tf.random_normal(shape=[n_nodes_hl2, n_nodes_hl3], dtype=tf.float32)),\n",
    "                         'biases': tf.Variable(tf.random_normal(shape=[n_nodes_hl3], dtype=tf.float32))}\n",
    "    \n",
    "    #output hidden layer\n",
    "    output_layer = {'weights':  tf.Variable(tf.random_normal(shape=[n_nodes_hl3, n_classes])),\n",
    "                         'biases': tf.Variable(tf.random_normal(shape=[n_classes]))}\n",
    "    \n",
    "    \n",
    "    \n",
    "    #model input_data * weight + biases\n",
    "    l1 = tf.add(tf.matmul(data, hidden_1_layer['weights']), hidden_1_layer['biases'])\n",
    "    l1 = tf.nn.relu(l1)\n",
    "    \n",
    "    l2 = tf.add(tf.matmul(l1, hidden_2_layer['weights']), hidden_2_layer['biases'])\n",
    "    l2 = tf.nn.relu(l2)\n",
    "    \n",
    "    l3 = tf.add(tf.matmul(l2, hidden_3_layer['weights']), hidden_3_layer['biases'])\n",
    "    l3 = tf.nn.relu(l3)\n",
    "    \n",
    "    output = tf.matmul(l3, output_layer['weights']) +  output_layer['biases']\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2e7f04a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_neural_network(x):\n",
    "    prediction = neural_network_model(x)\n",
    "    #tf.nn.softmax_cross_entropy_with_logits_v2(logits=Z3, labels=Y)\n",
    "    cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits_v2(prediction, y))\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "    \n",
    "    #cycles feed forward + back prop\n",
    "    hm_epochs = 10\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        \n",
    "        # make an iterator\n",
    "        train_iterator = tf.data.make_initializable_iterator(mnist_train)\n",
    "        next_element = train_iterator.get_next()\n",
    "        \n",
    "        \n",
    "        for epoch in range(hm_epochs):\n",
    "            opoch_loss = 0\n",
    "            \n",
    "            sess.run(train_iterator.initializer)\n",
    "            epoch_x, epoch_y = sess.run(next_element)\n",
    "            \n",
    "            for _ in range(int(train_info.splits[\"train\"].num_examples/batch_size)):\n",
    "                #sess.run(optimizer, feed_dict={x: batch_train_x, y: batch_train_y})\n",
    "                epoch_x, epoch_y = sess.run(x_train.train.next_batch(batch_size))\n",
    "                #epoch_x, epoch_y = mnist_train.batch(batch_size)\n",
    "                #epoch_x, epoch_y = next_batch(batch_size, mnist_train, labels)\n",
    "                _, c = sess.run([optimizer, cost], feed_dict = {x : epoch_x, y : epoch_y})\n",
    "                epoch_loss += c \n",
    "                \n",
    "            print(\"Epoch\", epoch, 'completed out of', hm_epochs, 'loss:', epoch_loss)\n",
    "        \n",
    "        correct = tf.equal(tf.argmax(prediction,1), tf.argmax(y,1))\n",
    "        \n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "        print('accuracy:', accuracy.eval({x:mnist.images, y:mnist.test.labels}))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a81af46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3f346f12",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MapDataset' object has no attribute 'train'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-58-68d74426330c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#tf.enable_eager_execution()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain_neural_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-57-0bfdc5d18a9c>\u001b[0m in \u001b[0;36mtrain_neural_network\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_info\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplits\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"train\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_examples\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m                 \u001b[1;31m#sess.run(optimizer, feed_dict={x: batch_train_x, y: batch_train_y})\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m                 \u001b[0mepoch_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmnist_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m                 \u001b[1;31m#epoch_x, epoch_y = mnist_train.batch(batch_size)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m                 \u001b[1;31m#epoch_x, epoch_y = next_batch(batch_size, mnist_train, labels)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'MapDataset' object has no attribute 'train'"
     ]
    }
   ],
   "source": [
    "#tf.enable_eager_execution() \n",
    "train_neural_network(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "affa638c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable_5:0' shape=(5,) dtype=float32_ref>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initializer = tf.random_normal_initializer()\n",
    "tf.Variable(initializer(shape=[5], dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "55e72420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((?, 28, 28, 1), (?, 10)), types: (tf.uint8, tf.float32)>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_train.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db66b205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:247: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:247: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Iterator' object has no attribute 'batch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-f1dda3162a6b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;31m# make an iterator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mtrain_iterator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_initializable_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmnist_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mnext_element\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmnist_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhm_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Iterator' object has no attribute 'batch'"
     ]
    }
   ],
   "source": [
    "prediction = neural_network_model(x)\n",
    "#tf.nn.softmax_cross_entropy_with_logits_v2(logits=Z3, labels=Y)\n",
    "cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits_v2(prediction, y))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "#cycles feed forward + back prop\n",
    "hm_epochs = 10\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "\n",
    "    # make an iterator\n",
    "    train_iterator = tf.data.make_initializable_iterator(mnist_train)\n",
    "    next_element = train_iterator.get_next(mnist_train, batch_size = batch_size)\n",
    "\n",
    "    for epoch in range(hm_epochs):\n",
    "        opoch_loss = 0\n",
    "\n",
    "        sess.run(train_iterator.initializer)\n",
    "        epoch_x, epoch_y = sess.run(next_element)\n",
    "\n",
    "        for _ in range(int(train_info.splits[\"train\"].num_examples/batch_size)):\n",
    "            #sess.run(optimizer, feed_dict={x: batch_train_x, y: batch_train_y})\n",
    "            #epoch_x, epoch_y = dataset[\"train\"].batch(batch_size)\n",
    "            _, c = sess.run([optimizer, cost], feed_dict = {x : epoch_x, y : epoch_y})\n",
    "            epoch_loss += c \n",
    "\n",
    "        print(\"Epoch\", epoch, 'completed out of', hm_epochs, 'loss:', epoch_loss)\n",
    "\n",
    "    correct = tf.equal(tf.argmax(prediction,1), tf.argmax(y,1))\n",
    "\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "    print('accuracy:', accuracy.eval({x:mnist.images, y:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d375db6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
